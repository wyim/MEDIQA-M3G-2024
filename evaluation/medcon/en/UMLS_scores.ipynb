{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "def read_json( file_path ) :\n",
    "    with open( file_path, 'r' ) as f :\n",
    "        return json.load( f )\n",
    "\n",
    "\n",
    "# the following code will only touch the prediction files containing the target_model_name. \n",
    "# to include all models, set the target_model_name as ''\n",
    "# this assumes you prediction files have this modelname in their name\n",
    "target_model_name='YOUR-SYSTEM-MODEL'\n",
    "\n",
    "reference_dir = 'YOUR-REFERENCE-DIR' #where you put the reference file\n",
    "prediction_dir = 'YOUR-PREDICTION-DIR' #where you put your prediction e.g. gpt4vision-run1.json\n",
    "score_dir = 'YOUR-SCORE-DIR' #where to output the score.json file\n",
    "intermediate_dir = 'YOUR-INTERMEDIA_DIR-DIR' #where to store the intermediate files for assertion labels\n",
    "\n",
    "# the directory where the UMLS concepts are installed.\n",
    "#to setup QUICKUMLS, please see directions from: https://github.com/Georgetown-IR-Lab/QuickUMLS\n",
    "quickumls_fp='YOUR-QUICKUMLS-INSTALLATION'\n",
    "UMLS_stop_words=read_json( os.path.join(reference_dir, f'UMLS_stop_words.json') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting UMLS concepts from the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 00:00:14.273124: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 00:00:14.460214: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-07 00:00:15.038602: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64\n",
      "2024-03-07 00:00:15.038668: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64\n",
      "2024-03-07 00:00:15.038674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# configuring QuickUMLS\n",
    "\n",
    "UMLS_semantics_types=['T116', 'T195', 'T123', 'T122', 'T200', 'T196', 'T126', 'T131', 'T125', 'T129', 'T130', 'T197', 'T114', 'T109', 'T121', 'T127', 'T020', 'T190', 'T049', 'T019', 'T047', 'T050', 'T033', 'T037', 'T048', 'T191', 'T046', 'T184', 'T060', 'T059', 'T063', 'T061']\n",
    "UMLS_semantics_names=['Amino Acid, Peptide, or Protein', 'Antibiotic', 'Biologically Active Substance', 'Biomedical or Dental Material', 'Clinical Drug', 'Element, Ion, or Isotope', 'Enzyme', 'Hazardous or Poisonous Substance', 'Hormone', 'Immunologic Factor', 'Indicator, Reagent, or Diagnostic Aid', 'Inorganic Chemical', 'Nucleic Acid, Nucleoside, or Nucleotide', 'Organic Chemical', 'Pharmacologic Substance', 'Vitamin', 'Acquired Abnormality', 'Anatomical Abnormality', 'Cell or Molecular Dysfunction', 'Congenital Abnormality', 'Disease or Syndrome', 'Experimental Model of Disease', 'Finding', 'Injury or Poisoning', 'Mental or Behavioral Dysfunction', 'Neoplastic Process', 'Pathologic Function', 'Sign or Symptom', 'Diagnostic Procedure', 'Laboratory Procedure', 'Molecular Biology Research Technique', 'Therapeutic or Preventive Procedure']\n",
    "UMLS_type_map={\n",
    "    \"Treatment\": [\"Amino Acid, Peptide, or Protein\", \"Antibiotic\", \"Biologically Active Substance\",\n",
    "        \"Biomedical or Dental Material\", \"Chemical\", \"Chemical Viewed Functionally\",\n",
    "        \"Chemical Viewed Structurally\", \"Clinical Drug\", \"Element, Ion, or Isotope\",\n",
    "        \"Enzyme\",  \"Hazardous or Poisonous Substance\", \"Hormone\",\n",
    "        \"Immunologic Factor\", \"Indicator, Reagent, or Diagnostic Aid\",\n",
    "        \"Inorganic Chemical\", \"Nucleic Acid, Nucleoside, or Nucleotide\",\n",
    "        \"Organic Chemical\", \"Pharmacologic Substance\",\n",
    "        \"Receptor\", \"Vitamin\", \"Therapeutic or Preventive Procedure\"],\n",
    "    \"Disease\": [\"Acquired Abnormality\", \"Anatomical Abnormality\",\n",
    "        \"Cell or Molecular Dysfunction\", \"Congenital Abnormality\",\n",
    "        \"Disease or Syndrome\", \"Experimental Model of Disease\", \"Finding\", \"Injury or Poisoning\",\n",
    "        \"Mental or Behavioral Dysfunction\", \"Neoplastic Process\", \"Pathologic Function\", \"Sign or Symptom\"],\n",
    "    \"Test\": [ \"Diagnostic Procedure\", \"Laboratory Procedure\", \"Molecular Biology Research Technique\"],\n",
    "}\n",
    "\n",
    "from quickumls import QuickUMLS\n",
    "matcher = QuickUMLS(quickumls_fp,window=5,threshold=0.9,accepted_semtypes=UMLS_semantics_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:14<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# extracting concepts from QuickUMLS\n",
    "normalized_concepts_dir='final_UMLS_sets.json'\n",
    "normalized_concepts=read_json(normalized_concepts_dir)\n",
    "\n",
    "\n",
    "for exp in tqdm(glob(f'{prediction_dir}/*.json')):\n",
    "    if target_model_name in exp:\n",
    "        prediction = read_json(exp)\n",
    "        for UMLS_source in ['UMLS_en']:\n",
    "                response_key=\"content_en\" if \"en\" in UMLS_source else \"content_zh\" #\"content_zh2en\"\n",
    "                for idx,response in enumerate(prediction):\n",
    "                    prediction[idx][\"responses\"][0][UMLS_source]=[]\n",
    "                    text=response[\"responses\"][0][response_key]\n",
    "                    matches=matcher.match(text, ignore_syntax=True)\n",
    "                    for match in matches:\n",
    "                        dic={\n",
    "                            'term':[],\n",
    "                            'cui':[],\n",
    "                            'semantic_types':[],\n",
    "                            \"type\":\"\",\n",
    "                            'index':[]\n",
    "                        }\n",
    "                        for m in match:\n",
    "                            if m['cui'] not in UMLS_stop_words['cuis'] and \\\n",
    "                                m['term'] not in UMLS_stop_words['terms'] and \\\n",
    "                                any([w in  UMLS_semantics_types for w in m['semtypes']]) :\n",
    "                                current_term= m['term'].lower()\n",
    "                                for k,v in normalized_concepts.items():\n",
    "                                    if current_term in v[\"concepts\"] or m['cui'] in v[\"cui\"]:\n",
    "                                        current_term=k\n",
    "                                        #print(exp,response['encounter_id'],m['term'].lower(),current_term)\n",
    "                                        break\n",
    "                                if current_term not in dic['term']:\n",
    "                                    dic['term'].append(current_term)\n",
    "                                if m['cui'] not in dic['cui']:\n",
    "                                    dic['cui'].append(m['cui'])\n",
    "                                if (m['start'],m['end']) not in dic['index']:\n",
    "                                    dic['index'].append((m['start'],m['end']))\n",
    "                                for w in m['semtypes']:\n",
    "                                    if w in  UMLS_semantics_types:\n",
    "                                        st=UMLS_semantics_names[UMLS_semantics_types.index(w)] \n",
    "                                        if st not in dic['semantic_types']:\n",
    "                                            dic['semantic_types'].append(st)\n",
    "                                if not dic['type']:\n",
    "                                    dic['type']=[k for k,v in UMLS_type_map.items() if not set(dic['semantic_types']).isdisjoint(v)][0]\n",
    "                            \n",
    "                        if dic['type']!=\"\":\n",
    "                            prediction[idx][\"responses\"][0][UMLS_source].append(copy.deepcopy(dic))\n",
    "        with open(exp,'w') as f:\n",
    "            json.dump(prediction,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating assertion labesl for those concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "for file in glob('{}/*.json'.format(prediction_dir)):\n",
    "    if target_model_name not in file:\n",
    "        continue\n",
    "    id=file.split('/')[-1]\n",
    "    dics=read_json(file)\n",
    "    for dic in dics:\n",
    "        for idx,r in enumerate(dic['responses']):\n",
    "            if r[\"UMLS_en\"]:\n",
    "                text=list(r[\"content_en\"])\n",
    "                for concept in r[\"UMLS_en\"]:\n",
    "                    text[concept[\"index\"][0][0]]=\"<\"+text[concept[\"index\"][0][0]]\n",
    "                    text[concept[\"index\"][0][1]-1]=text[concept[\"index\"][0][1]-1]+f\">({concept['type']})\"\n",
    "\n",
    "                output.append({\n",
    "                    \"id\": '[SEP]'.join([id,dic['encounter_id'],str(idx)]),\n",
    "                    \"instruction\": \"Decide the status value for each medical problem event. Choose from present, absent, possible, conditional, hypothetical, not_patient.\",\n",
    "                    \"input\": ''.join(text),\n",
    "                    \"output\": '[SEP]'.join([id,dic['encounter_id'],str(idx)])\n",
    "                })\n",
    "with open(f'{intermediate_dir}/{target_model_name}_assertion_input.json','w') as f:\n",
    "    json.dump(output,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE ASSERTION CLASSIFIER using the instruction above and output to a folder \"intermediate_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comedo': 'present',\n",
       " 'acne': 'present',\n",
       " 'blockage': 'present',\n",
       " 'topical creams': 'hypothetical',\n",
       " 'benzoyl peroxide': 'hypothetical',\n",
       " 'salicylic acid': 'hypothetical',\n",
       " 'exfoliation': 'present',\n",
       " 'dermatological': 'hypothetical',\n",
       " 'procedure': 'hypothetical'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the result to \n",
    "assertion_labels={}\n",
    "for assertion_file in glob(f'{intermediate_dir}/*assertion_output.jsonl'):\n",
    "    for line in open(assertion_file).readlines():\n",
    "        if line.strip():\n",
    "            dic=json.loads(line)\n",
    "            assertion_labels[dic['label']]={}\n",
    "            for w in dic['predict'].split('[SEP]'):\n",
    "                if w.strip() and '<status>' in w and w.split('<status>')[1].strip() in ['present', 'absent', 'possible', 'conditional','hypothetical', 'not_patient']:\n",
    "                    key,status=w.split('<status>')\n",
    "                    assertion_labels[dic['label']][key.strip().lower()]=status.strip()\n",
    "assertion_labels[dic['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the assertion output, add the file\n",
    "for file in glob('{}/*.json'.format(prediction_dir)):\n",
    "    if target_model_name not in file:\n",
    "        continue\n",
    "    dics=read_json(file)\n",
    "    for dic in dics:\n",
    "        for idx,r in enumerate(dic['responses']):\n",
    "            if r[\"UMLS_en\"]:\n",
    "                id=file.split('/')[-1]\n",
    "                id='[SEP]'.join([id,dic['encounter_id'],str(idx)])\n",
    "                for concept in r[\"UMLS_en\"]:\n",
    "                    concept_key=r[\"content_en\"][concept[\"index\"][0][0]:concept[\"index\"][0][1]].strip().lower()\n",
    "                    concept['status']=assertion_labels[id].get(concept_key,'present')\n",
    "    with open(file,'w') as f:\n",
    "        json.dump(dics,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating the UMLS set scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in  ['UMLS_en']:\n",
    "    for file in glob(f'{prediction_dir}/*.json'):\n",
    "        exp=file.split('/')[-1].split('.')[0]\n",
    "        task='iiyi_test' if 'iiyi' in exp else 'reddit_test'\n",
    "\n",
    "        truth = read_json( os.path.join(reference_dir, f'{task}.json') )\n",
    "        prediction = read_json( os.path.join(prediction_dir, f'{exp}.json') )\n",
    "\n",
    "        score_path = os.path.join(score_dir, f'{exp}.json')\n",
    "        scores={} if not os.path.isfile(score_path) else read_json(score_path)\n",
    "\n",
    "        all_scores=[]\n",
    "        for pred, ref in zip(prediction,truth):\n",
    "            assert pred[\"encounter_id\"]==ref[\"encounter_id\"],1\n",
    "            NP=len(pred)\n",
    "            max_F1=0\n",
    "            \n",
    "            for p in pred['responses']:\n",
    "                p=set([(c[\"term\"][0],c[\"type\"],c.get('status','present')) for c in p[lang]])\n",
    "\n",
    "                for r in ref['responses']:\n",
    "                    r=set([(c['UMLS_term'],c[\"type\"],c.get('status','present')) for c in r[lang]])\n",
    "\n",
    "                    NT=len(r)\n",
    "                    TP=len([t for t in p if t in r])\n",
    "                    P=TP/NP if NP else 0\n",
    "                    R=TP/NT if NT else 0\n",
    "                    F1=R*P*2/(P+R) if P+R else 0\n",
    "                    max_F1=max(max_F1,F1)\n",
    "                    \n",
    "            pred[lang+' F1']=max_F1\n",
    "            all_scores.append(max_F1)\n",
    "\n",
    "        with open(file,'w') as f:\n",
    "            json.dump(prediction,f,indent=4)\n",
    "\n",
    "        scores[lang+' F1'] = np.mean(all_scores)\n",
    "        with open(score_path,'w') as f:\n",
    "            json.dump(scores,f,indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
